# -*- coding: utf-8 -*-
"""fraud_Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xvaXzXYIr4LT68NwAXLpgbfS8i2XOHpG
"""

from google.colab import files

files.upload()  # select the kaggle.json file that you downloaded

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d mlg-ulb/creditcardfraud

!unzip creditcardfraud.zip

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE

# Load the dataset
data = pd.read_csv('creditcard.csv')

# Rename V1-V28 for better understanding
for i in range(1, 29):
    data.rename(columns={f'V{i}': f'Component {i}'}, inplace=True)

# Handle missing values if any
# Assuming no missing values in this dataset as per the UCI ML repository.

# Scale the 'Time' and 'Amount' features
scaler = StandardScaler()
data['Normalized Amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))
data['Normalized Time'] = scaler.fit_transform(data['Time'].values.reshape(-1, 1))

# Drop the original 'Time' and 'Amount' features
data.drop(['Time', 'Amount'], axis=1, inplace=True)

# Split the data into features and targets
X = data.drop('Class', axis=1)
y = data['Class']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Since RandomForest is a good starting point and often suggested for imbalanced datasets, we will use that.
# Create the model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Oversample the minority class using SMOTE
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

# Fit the model
model.fit(X_train_res, y_train_res)

# Predict on the test data
y_pred = model.predict(X_test)

# Print the classification report and confusion matrix
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import SVC
import joblib
from sklearn.linear_model import LinearRegression
from sklearn.metrics import classification_report, confusion_matrix
# Load the dataset
data = pd.read_csv('creditcard.csv')

# Rename V1-V28 for better understanding
for i in range(1, 29):
    data.rename(columns={f'V{i}': f'Component {i}'}, inplace=True)

# Handle missing values if any
# Assuming no missing values in this dataset as per the UCI ML repository.

# Scale the 'Time' and 'Amount' features
scaler = StandardScaler()
data['Normalized Amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))
data['Normalized Time'] = scaler.fit_transform(data['Time'].values.reshape(-1, 1))

# Drop the original 'Time' and 'Amount' features
data.drop(['Time', 'Amount'], axis=1, inplace=True)

# Split the data into features and targets
X = data.drop('Class', axis=1)
y = data['Class']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(data.head())

# Train and evaluate each model
import os
# Define the models
models = {
    "Linear Regression": LinearRegression(),
    "Logistic Regression": LogisticRegression(),
    "Quadratic Regression": make_pipeline(PolynomialFeatures(2), LinearRegression()),
    "Naive Bayes": GaussianNB(),
    "KNN": KNeighborsClassifier(),
    "MLP": MLPClassifier(max_iter=1000),
    "LDA": LinearDiscriminantAnalysis(),
    "QDA": QuadraticDiscriminantAnalysis(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC(),
    "AdaBoost": AdaBoostClassifier()
}


for name, model in models.items():
    model.fit(X_train, y_train)

    # Predict with the model
    y_pred = model.predict(X_test)

    # Handle continuous predictions for Linear and Quadratic Regression
    if name in ["Linear Regression", "Quadratic Regression"]:
        # Apply threshold to get binary class labels
        y_pred_binary = np.where(y_pred > 0.5, 1, 0)
        print(f"Results for {name}:")
        print(classification_report(y_test, y_pred_binary))
        print(confusion_matrix(y_test, y_pred_binary))
    else:
        print(f"Results for {name}:")
        print(classification_report(y_test, y_pred))
        print(confusion_matrix(y_test, y_pred))

    # Save the model
    joblib.dump(model, f'models/{name}_model.pkl')

# Generate synthetic data for testing
synthetic_data = []
num_samples_per_class = 10  # 10 samples for each class

for class_label in [0, 1]:
    for _ in range(num_samples_per_class):
        # Randomly create a new sample
        synthetic_sample = {f'Component {i}': np.random.normal() for i in range(1, 29)}
        synthetic_sample['Normalized Amount'] = np.random.normal()
        synthetic_sample['Normalized Time'] = np.random.normal()
        synthetic_sample['Class'] = class_label
        synthetic_data.append(synthetic_sample)

# Convert to DataFrame
synthetic_df = pd.DataFrame(synthetic_data)

# Display the first few synthetic data samples
synthetic_df.head()

# Load the best model
best_model_loaded = joblib.load('/content/models/Random Forest_model.pkl')

# Evaluate on test data
y_pred_best = best_model_loaded.predict(X_test)
print(classification_report(y_test, y_pred_best))
print(confusion_matrix(y_test, y_pred_best))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
best_model_loaded = joblib.load('/content/models/Random Forest_model.pkl')
# Predicting using the best model (Random Forest in this case)
y_pred_best = best_model_loaded.predict(X_test)

# Calculating metrics
accuracy = accuracy_score(y_test, y_pred_best)
precision = precision_score(y_test, y_pred_best)
recall = recall_score(y_test, y_pred_best)
f1 = f1_score(y_test, y_pred_best)

# Confusion matrix for calculating IBA
tn, fp, fn, tp = confusion_matrix(y_test, y_pred_best).ravel()

# Calculate the Index of Balanced Accuracy (IBA)
balance = (fn + tp) / (fp + tn)
iba = 1 + (recall - 0.5) * balance

# Print the metrics
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Index of Balanced Accuracy (IBA): {iba}")

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred_best)
precision = precision_score(y_test, y_pred_best)
recall = recall_score(y_test, y_pred_best)
f1 = f1_score(y_test, y_pred_best)

# Confusion matrix for calculating IBA
tn, fp, fn, tp = confusion_matrix(y_test, y_pred_best).ravel()

# Calculate the Index of Balanced Accuracy (IBA)
dominance = (tp / (tp + fn)) - (tn / (tn + fp))
gmean = np.sqrt(tp / (tp + fn) * tn / (tn + fp))
iba = (1 + dominance) * gmean**2

# Print the metrics
print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Index of Balanced Accuracy (IBA): {iba}")

from imblearn.over_sampling import SMOTE

# Create a synthetic dataset with a 50-50 split of classes 0 and 1
smote = SMOTE()
X_synthetic, y_synthetic = smote.fit_resample(X, y)

# Split the synthetic data into training and test sets
X_train_synthetic, X_test_synthetic, y_train_synthetic, y_test_synthetic = train_test_split(X_synthetic, y_synthetic, test_size=0.3, random_state=42)

# Train the RandomForest model (which was previously named 'model') on the synthetic training data
model.fit(X_train_synthetic, y_train_synthetic)

# Evaluate on synthetic test data
y_pred_synthetic = model.predict(X_test_synthetic)
print("\nEvaluation on Synthetic Test Data:")
print(classification_report(y_test_synthetic, y_pred_synthetic))
print(confusion_matrix(y_test_synthetic, y_pred_synthetic))

# Calculate and print evaluation metrics for synthetic test data
accuracy_synthetic = accuracy_score(y_test_synthetic, y_pred_synthetic)
precision_synthetic = precision_score(y_test_synthetic, y_pred_synthetic)
recall_synthetic = recall_score(y_test_synthetic, y_pred_synthetic)
f1_synthetic = f1_score(y_test_synthetic, y_pred_synthetic)

# Confusion matrix for calculating IBA for synthetic test data
tn_s, fp_s, fn_s, tp_s = confusion_matrix(y_test_synthetic, y_pred_synthetic).ravel()

# Calculate IBA for synthetic test data
dominance_s = (tp_s / (tp_s + fn_s)) - (tn_s / (tn_s + fp_s))
gmean_s = np.sqrt(tp_s / (tp_s + fn_s) * tn_s / (tn_s + fp_s))
iba_s = (1 + dominance_s) * gmean_s**2

# Print the metrics for synthetic test data
print(f"Synthetic Test Data - Accuracy: {accuracy_synthetic}")
print(f"Synthetic Test Data - Precision: {precision_synthetic}")
print(f"Synthetic Test Data - Recall: {recall_synthetic}")
print(f"Synthetic Test Data - F1 Score: {f1_synthetic}")
print(f"Synthetic Test Data - Index of Balanced Accuracy (IBA): {iba_s}")

print("\nEvaluation on Synthetic Test Data:")
print("Is the model good at identifying fraud? Let's see:")

print(f"Overall Correctness (Accuracy): {accuracy_synthetic:.2f}")
print(f"Correctness when Identifying Fraud (Precision): {precision_synthetic:.2f}")
print(f"Ability to Catch Fraud (Recall): {recall_synthetic:.2f}")
print(f"Overall Score for Identifying Fraud (F1 Score): {f1_synthetic:.2f}")
print(f"Adjusted Score for Fairness (IBA): {iba_s:.2f}")

!ls /content/models/

!pip install joblib

from google.colab import drive
drive.mount('/content/drive')

# Load necessary libraries
import joblib
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import os

# Define your model filenames
model_filenames = [
    'AdaBoost_model.pkl', 'Decision Tree_model.pkl', 'KNN_model.pkl',
    'LDA_model.pkl', 'Linear Regression_model.pkl', 'Logistic Regression_model.pkl',
    'MLP_model.pkl', 'Naive Bayes_model.pkl', 'QDA_model.pkl', 'Random Forest_model.pkl'
]

# Dictionary to hold the results
results = {}

# Loop through each model, load it, make predictions, and store performance metrics
for filename in model_filenames:
    try:
        # Extract the model name from the filename
        model_name = filename.replace('_model.pkl', '').replace('_', ' ')

        # Load the trained model from the specified path
        model = joblib.load(f'/content/drive/MyDrive/hackathon_Canara/{filename}')

        # Predict with the model
        # If the model supports `predict_proba`, use it to get binary predictions
        if hasattr(model, 'predict_proba'):
            y_pred_prob = model.predict_proba(X_test)[:,1]
            y_pred = (y_pred_prob >= 0.5).astype(int)
        else:
            y_pred = model.predict(X_test)

        # Check if y_test is binary; this is crucial for the following metrics to work
        if set(np.unique(y_test)) != {0, 1}:
            raise ValueError("y_test must be binary for these metrics.")

        # Calculate and store each metric
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='binary')
        recall = recall_score(y_test, y_pred, average='binary')
        f1 = f1_score(y_test, y_pred, average='binary')
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
        gmean = np.sqrt(tp / (tp + fn) * tn / (tn + fp))
        dominance = (tp / (tp + fn)) - (tn / (tn + fp))
        iba = (1 + dominance) * gmean**2

        results[model_name] = {
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1 Score': f1,
            'IBA': iba
        }
    except ValueError as e:
        print(f"Skipping model {model_name} due to error: {e}")

# Convert the results dictionary to a DataFrame
df = pd.DataFrame(results).T

# Sort the DataFrame by F1 Score for the first figure
df_sorted_by_f1 = df.sort_values('F1 Score', ascending=True)

# Plotting the F1 Score and IBA for models
plt.figure(figsize=(10, 8))
df_sorted_by_f1[['F1 Score', 'IBA']].plot(kind='barh', color=['blue', 'orange'])
plt.title('F1 Score and IBA for Different Models')
plt.xlabel('Score')
plt.ylabel('Model')
plt.tight_layout()
plt.show()

# Sort the DataFrame by Accuracy for the second figure
df_sorted_by_accuracy = df.sort_values('Accuracy', ascending=True)

# Plotting the Accuracy and IBA for models
plt.figure(figsize=(10, 8))
df_sorted_by_accuracy[['Accuracy', 'IBA']].plot(kind='barh', color=['blue', 'orange'])
plt.title('Accuracy and IBA for Different Models')
plt.xlabel('Score')
plt.ylabel('Model')
plt.tight_layout()
plt.show()

# Select and evaluate the best model based on F1 Score
# Find the model with the highest F1 Score
best_model_name = df['F1 Score'].idxmax()  # Get the index (model name) of the max F1 Score
best_model_path = f'/content/drive/MyDrive/hackathon_Canara/best_model.pkl'
best_model = joblib.load(best_model_path)

# Now, you can use the best_model to predict and evaluate on the test set
y_pred = best_model.predict(X_test)
print(f"Classification Report for the Best Model ({best_model_name}):")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

!pip install matplotlib imbalanced-lear

# Install necessary libraries
from google.colab import files
files.upload()  # Upload the Kaggle API key
!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download the dataset
!kaggle datasets download -d mlg-ulb/creditcardfraud
!unzip creditcardfraud.zip

data = pd.read_csv('creditcard.csv')

# Filter the data to only include rows where Class is 1
fraudulent_transactions = data[data['Class'] == 1]

# Set pandas options to display all columns
pd.set_option('display.max_columns', None)  # Ensures all columns are displayed
pd.set_option('display.width', 1000)        # Sets the display width to avoid wrapping

# Print a sample of the fraudulent transactions
print(fraudulent_transactions.sample())

# Load the dataset
from google.colab import files
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from sklearn.metrics import precision_recall_curve, f1_score, auc, confusion_matrix
import numpy as np
data = pd.read_csv('creditcard.csv')

# Rename V1-V28 for better understanding
for i in range(1, 29):
    data.rename(columns={f'V{i}': f'Component {i}'}, inplace=True)

# Handle missing values if any (Assuming no missing values)

# Scale the 'Time' and 'Amount' features
scaler = StandardScaler()
data['Normalized Amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))
data['Normalized Time'] = scaler.fit_transform(data['Time'].values.reshape(-1, 1))

# Drop the original 'Time' and 'Amount' features
data.drop(['Time', 'Amount'], axis=1, inplace=True)

# Split the data into features (X) and targets (y)
X = data.drop('Class', axis=1)
y = data['Class']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Create a pipeline with resampling and model fitting
pipeline = Pipeline([
    ('over', SMOTE(sampling_strategy=0.1, random_state=42)),
    ('under', RandomUnderSampler(sampling_strategy=0.5, random_state=42)),
    ('model', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))
])

# Fit the model
pipeline.fit(X_train, y_train)

# Predict on the test data
y_pred = pipeline.predict(X_test)

# Evaluate the model
precision, recall, _ = precision_recall_curve(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
auprc = auc(recall, precision)
specificity = np.sum((y_test == 0) & (y_pred == 0)) / np.sum(y_test == 0)

print(f'F1 Score: {f1}')
print(f'AUPRC: {auprc}')
print(f'Specificity: {specificity}')

# Creating synthetic data for testing
from sklearn.ensemble import RandomForestClassifier

smote = SMOTE(random_state=42)
X_synthetic, y_synthetic = smote.fit_resample(X_train, y_train)
model_synthetic = RandomForestClassifier(n_estimators=100, random_state=42)
model_synthetic.fit(X_synthetic, y_synthetic)
y_pred_synthetic = model_synthetic.predict(X_test)
print(classification_report(y_test, y_pred_synthetic))
print(confusion_matrix(y_test, y_pred_synthetic))

!pip install -U imbalanced-learn

# Install TensorFlow (Google Colab should have TensorFlow preinstalled)
# !pip install tensorflow

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.utils import to_categorical

# Load the dataset
data = pd.read_csv('creditcard.csv')

# Rename V1-V28 for better understanding
for i in range(1, 29):
    data.rename(columns={f'V{i}': f'Component_{i}'}, inplace=True)

# Scale the 'Time' and 'Amount' features
scaler = StandardScaler()
data['Normalized Amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))
data['Normalized Time'] = scaler.fit_transform(data['Time'].values.reshape(-1, 1))

# Drop the original 'Time' and 'Amount' features
data.drop(['Time', 'Amount'], axis=1, inplace=True)

# Split the data into features (X) and targets (y) with stratified sampling
X = data.drop('Class', axis=1)
y = data['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define SMOTE for resampling
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Build the neural network model
model = models.Sequential()
model.add(layers.Dense(256, activation='relu', input_shape=(X_resampled.shape[1],)))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))  # Output layer

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Calculate class weights for imbalanced dataset
class_weights = {
    0: (1 / y_train.value_counts()[0]) * (len(y_train) / 2.0),
    1: (1 / y_train.value_counts()[1]) * (len(y_train) / 2.0)
}

# Train the model with GPU acceleration
history = model.fit(X_resampled, y_resampled,
                    epochs=10,
                    batch_size=512,
                    class_weight=class_weights,
                    validation_data=(X_test, y_test))

# Evaluate the model on the test data
test_loss, test_acc = model.evaluate(X_test, y_test)
print('Test accuracy:', test_acc)

# Generate classification report
y_pred = model.predict(X_test) > 0.5  # Convert probabilities to binary output
print(classification_report(y_test, y_pred))

model.save('best_fraud_detection_model.h5')

# Optional: Generate synthetic data to test model performance
smote_test = SMOTE(random_state=42)
X_synthetic, y_synthetic = smote_test.fit_resample(X_test, y_test)
synthetic_loss, synthetic_acc = model.evaluate(X_synthetic, y_synthetic)
print('Synthetic data test accuracy:', synthetic_acc)

# Plotting the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print(conf_matrix)

# Plot training history
pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
plt.show()

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

plt.tight_layout()
plt.show()

# Load the trained model
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.utils import to_categorical
model = tf.keras.models.load_model('/content/drive/MyDrive/hackathon_Canara/best_fraud_detection_model.h5')

# Scale features except for 'Class'
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data.drop('Class', axis=1))

# Add the scaled features back into the dataframe
for i, col in enumerate(data.columns.drop('Class')):
    data[col] = X_scaled[:, i]

# Filter out a fraud sample
fraud_sample = data[data['Class'] == 1].drop('Class', axis=1).iloc[0]

# Scale the features of the fraud sample
scaled_fraud_sample = scaler.transform([fraud_sample.values])

# Predict using the trained model
fraud_prediction = model.predict(scaled_fraud_sample)
fraud_status = 'Fraud' if fraud_prediction[0] > 0.5 else 'Not Fraud'

print("Predicted class for the selected fraud sample:", fraud_status)

# Make a prediction
# Select a random sample from the fraudulent transactions and drop the 'Class' column
sample_for_prediction = fraudulent_transactions.drop('Class', axis=1).sample()

# Make sure the sample is transformed/scaled in the same way as the training data
# (Assuming you have a scaler object from when you trained your model)
scaled_sample_for_prediction = scaler.transform(sample_for_prediction)

# Make a prediction
prediction = model.predict(scaled_sample_for_prediction)

# Interpret the prediction result
fraud_status = 'Fraud' if prediction[0] > 0.5 else 'Not Fraud'
print("Predicted class for the selected fraud sample:", fraud_status)

!pip install sweetviz

import pandas as pd
import sweetviz as sv
data = pd.read_csv('creditcard.csv')

report = sv.analyze(data)
report.show_html('Report.html')